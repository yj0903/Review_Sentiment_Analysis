{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fashion = pd.read_csv(\"../data/01. 패션.csv\", encoding='utf-8', sep=\",\")\n",
    "df_cosmetic = pd.read_csv(\"../data/02. 화장품.csv\", encoding='utf-8', sep=\",\")\n",
    "df_appliance = pd.read_csv(\"../data/03. 가전.csv\", encoding='utf-8', sep=\",\")\n",
    "df_it = pd.read_csv(\"../data/04. IT기기.csv\", encoding='utf-8', sep=\",\")\n",
    "\n",
    "result = pd.concat([df_fashion, df_cosmetic, df_appliance, df_it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReviewScore 80, 4 이상 긍정적, 50이하, 2이하 부정적으로 지정\n",
    "\n",
    "def tokenize_korean_text(text): \n",
    "  text_filtered = re.sub('[^,.?!\\w\\s]','', text)\n",
    "\n",
    "  okt = Okt() \n",
    "  Okt_morphs = okt.pos(text_filtered) \n",
    "\n",
    "  words = []\n",
    "  for word, pos in Okt_morphs:\n",
    "    if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun':\n",
    "      words.append(word)\n",
    "\n",
    "  words_str = ' '.join(words)\n",
    "  return words_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label 은 평점 기준으로 긍정이면 1, 부정이면 0으로 라벨링 지정\n",
    "\n",
    "#### 학습용 데이터로 가공\n",
    "- 평점 8 이상 혹은 3 이하만 저장 (8 이상: 긍정적, 3 이하: 부정적)\n",
    "- 각 text를 tokenize한 후, 동사, 형용사, 명사만 저장 (konlpy의 Okt 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_itdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m df_itdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore_change\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore_change\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReviewScore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m90\u001b[39m:\u001b[38;5;241m4.5\u001b[39m,\u001b[38;5;241m80\u001b[39m:\u001b[38;5;241m4.0\u001b[39m,\u001b[38;5;241m70\u001b[39m:\u001b[38;5;241m3.5\u001b[39m,\u001b[38;5;241m60\u001b[39m:\u001b[38;5;241m3.0\u001b[39m,\u001b[38;5;241m50\u001b[39m:\u001b[38;5;241m2.5\u001b[39m,\u001b[38;5;241m40\u001b[39m:\u001b[38;5;241m2.0\u001b[39m,\u001b[38;5;241m30\u001b[39m:\u001b[38;5;241m1.5\u001b[39m,\u001b[38;5;241m20\u001b[39m:\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m0\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_itdf' is not defined"
     ]
    }
   ],
   "source": [
    "df = df_it\n",
    "df['label'] = 0\n",
    "df['label'] = df['Score_change'].apply(lambda x: 1 if x > 3.5 else 0)\n",
    "\n",
    "df['Score_change'] = df['ReviewScore'].replace({100:5,90:4.5,80:4.0,70:3.5,60:3.0,50:2.5,40:2.0,30:1.5,20:1.0,10:0.5,0:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_texts = []\n",
    "y = []\n",
    "\n",
    "for Score_change, RawText in zip(df['Score_change'], df['RawText']):\n",
    "  tokenized_comment = tokenize_korean_text(RawText)  # 위에서 만들었던 함수로 comment 쪼개기\n",
    "  X_texts.append(tokenized_comment)\n",
    "  y.append(1 if Score_change > 3.5 else 0)\n",
    "\n",
    "# train_test_split\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(X_texts, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    39021\n",
       "0     2336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer로 vector화\n",
    "tf_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "X_train_tf = tf_vectorizer.fit_transform(X_train_texts)  # training data에 맞게 fit & training data를 transform\n",
    "X_test_tf = tf_vectorizer.transform(X_test_texts) # test data를 transform\n",
    "\n",
    "vocablist = [word for word, number in sorted(tf_vectorizer.vocabulary_.items(), key=lambda x:x[1])]  # 단어들을 번호 기준 내림차순으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(C=0.1, penalty='l2', random_state=0)\n",
    "logistic_model.fit(X_train_tf, y_train)  # 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 472 out of 8272\n",
      "Accuracy: 0.9429400386847195\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = logistic_model.predict(X_test_tf)\n",
    "\n",
    "print('Misclassified samples: {} out of {}'.format((y_test_pred != y_test).sum(), len(y_test)))\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    8192\n",
      "0      80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_pred_series = pd.Series(y_test_pred)\n",
    "value_counts = y_pred_series.value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)\n",
      "최고(0.830)\n",
      "좋아요(0.768)\n",
      "좋습니다(0.766)\n",
      "만족해요(0.743)\n",
      "좋네요(0.726)\n",
      "만족(0.669)\n",
      "만족합니다(0.660)\n",
      "만족스러워요(0.641)\n",
      "감사해요(0.625)\n",
      "감사합니다(0.604)\n",
      "\n",
      "부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)\n",
      "화나네요(-0.680)\n",
      "안되서(-0.684)\n",
      "별로(-0.695)\n",
      "불만족(-0.736)\n",
      "비추(-0.753)\n",
      "반품(-0.762)\n",
      "짜증나네요(-0.792)\n",
      "안되고(-0.818)\n",
      "실망(-1.053)\n",
      "최악(-1.086)\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_model.coef_.tolist()\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# coefficients(계수)가 큰 값부터 내림차순으로 정렬\n",
    "\n",
    "print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[:10]:\n",
    "  print('{0:}({1:.3f})'.format(vocablist[word_num], coef))\n",
    "\n",
    "print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[-10:]: \n",
    "  print('{0:}({1:.3f})'.format(vocablist[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정/부정 테스트용 함수 생성\n",
    "def guess_good_or_bad(model, text):\n",
    "    text_filtered = text.replace('.', '').replace(',', '').replace(\"'\", \"\").replace('·', ' ').replace('=', '')\n",
    "    okt = Okt()  # Corrected line\n",
    "    Okt_morphs = okt.pos(text_filtered)\n",
    "\n",
    "    words = []\n",
    "    for word, pos in Okt_morphs:\n",
    "        if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun':\n",
    "            words.append(word)\n",
    "    words_str = ' '.join(words)\n",
    "\n",
    "    # Assuming you have defined tf_vectorizer and model somewhere in your code\n",
    "    new_text_tf = tf_vectorizer.transform([words_str])\n",
    "    result = model.predict(new_text_tf)[0]\n",
    "\n",
    "    if result == 1:\n",
    "        print('긍정')\n",
    "    else:\n",
    "        print('부정')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정\n"
     ]
    }
   ],
   "source": [
    "guess_good_or_bad(logistic_model, '안되서 화나네요 최악입니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n"
     ]
    }
   ],
   "source": [
    "guess_good_or_bad(logistic_model, '좋아요 최고')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = 'logistic_model.joblib'\n",
    "vectorizer_filename = 'vectorizer.joblib'\n",
    "joblib.dump(logistic_model, model_filename)\n",
    "joblib.dump(tf_vectorizer, vectorizer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.940522243713733\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train_tf, y_train)  # 학습\n",
    "y_test_pred = naive_bayes_model.predict(X_test_tf)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    8052\n",
      "0     220\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_pred_series = pd.Series(y_test_pred)\n",
    "value_counts = y_pred_series.value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)\n",
      "감사해요: 3.818\n",
      "감사합니다: 3.467\n",
      "다니기: 2.876\n",
      "안심: 2.870\n",
      "편리하고: 2.822\n",
      "꼼꼼하게: 2.690\n",
      "매력: 2.654\n",
      "이쁩니다: 2.639\n",
      "효율: 2.570\n",
      "기뻐요: 2.562\n",
      "\n",
      "부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)\n",
      "받지도: -3.652\n",
      "보내라: -3.652\n",
      "빵점: -3.652\n",
      "억지로: -3.652\n",
      "인쇄물: -3.652\n",
      "참조: -3.652\n",
      "못잡고: -3.876\n",
      "성질: -3.876\n",
      "저장장치: -3.876\n",
      "콜센터: -3.876\n"
     ]
    }
   ],
   "source": [
    "# Access feature log probabilities\n",
    "feature_log_probs = naive_bayes_model.feature_log_prob_\n",
    "\n",
    "# Calculate the difference between positive and negative log probabilities\n",
    "log_prob_diff = feature_log_probs[1] - feature_log_probs[0]\n",
    "\n",
    "# Create a list of (word, log_prob_diff) tuples\n",
    "word_log_prob_diff = list(zip(vocablist, log_prob_diff))\n",
    "\n",
    "# Sort the list by log_prob_diff in descending order\n",
    "sorted_word_log_prob_diff = sorted(word_log_prob_diff, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)')\n",
    "for word, log_prob_diff in sorted_word_log_prob_diff[:10]:\n",
    "    print('{0}: {1:.3f}'.format(word, log_prob_diff))\n",
    "\n",
    "print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)')\n",
    "for word, log_prob_diff in sorted_word_log_prob_diff[-10:]:\n",
    "    print('{0}: {1:.3f}'.format(word, log_prob_diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n"
     ]
    }
   ],
   "source": [
    "guess_good_or_bad(naive_bayes_model, '좋아요 최고')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n"
     ]
    }
   ],
   "source": [
    "guess_good_or_bad(naive_bayes_model, '좋아요 최고')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = 'naive_bayes_model.joblib'\n",
    "vectorizer_filename = 'vectorizer.joblib'\n",
    "joblib.dump(naive_bayes_model, model_filename)\n",
    "joblib.dump(tf_vectorizer, vectorizer_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU를 사용할 수 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model 토크나이저\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#### 토큰화\n",
    "def encode(data, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    " \n",
    "    for text in data:\n",
    "        tokenized_text = tokenizer.encode_plus(text,\n",
    "                                            max_length=50,\n",
    "                                            add_special_tokens = True,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                              truncation=True)\n",
    "        \n",
    "        input_ids.append(tokenized_text['input_ids'])\n",
    "        attention_masks.append(tokenized_text['attention_mask'])\n",
    "        token_type_ids.append(tokenized_text['token_type_ids'])\n",
    "    \n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "### BERT 모델 입력을 위한 형태로 처리 \n",
    "\n",
    "#딕셔너리 형태로 변환해서 출력 \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "      }, label\n",
    "      \n",
    " #데이터를 BERT에 넣을 수 있는 형태로 변경 \n",
    "def data_encode(input_ids_list, attention_mask_list, token_type_ids_list, label_list):\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리\n",
    "- 평점에 따라 긍1, 부0으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Score_change'] = result['ReviewScore'].replace({100:5,90:4.5,80:4.0,70:3.5,60:3.0,50:2.5,40:2.0,30:1.5,20:1.0,10:0.5,0:0})\n",
    "result['label'] = result['Score_change'].apply(lambda x: 1 if x > 3.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['패션', '화장품', '가전', 'IT기기'], dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Domain'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 패션 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35996,) (9000,) (35996,) (9000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    28952\n",
       "0     7044\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = result[result['Domain']== '패션']\n",
    "# train_test_split\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['RawText'], df['label'], test_size=0.2, random_state=0)\n",
    "\n",
    "# 레이블을 정수형으로 변환\n",
    "y_train = [int(label) for label in y_train]\n",
    "y_test = [int(label) for label in y_test]\n",
    "\n",
    "# 레이블을 넘파이 배열로 변환\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train_texts.shape, X_test_texts.shape, y_train.shape, y_test.shape)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### encode 함수를 정의해서 토큰화를 실시한다. \n",
    "여기서는 token_type_ids 정보도 추출하는데, 이는 각 토큰의 문장 임베딩 정보를 포함하고 있다. 여기서는 리뷰가 한개씩 입력되지만, 원래 BERT모델은 두 개의 문장을 입력받기 때문에 동일한 구조로 사용하기 위해서 해당 정보도 추출한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "#학습데이터 토큰화\n",
    "train_input_ids, train_attention_masks, train_token_type_ids = encode(X_train_texts, tokenizer)\n",
    "#테스트데이터 토큰화\n",
    "test_input_ids, test_attention_masks, test_token_type_ids = encode(X_test_texts, tokenizer)\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "train_data_encoded = data_encode(train_input_ids, train_attention_masks, train_token_type_ids, y_train).shuffle(10000).batch(BATCH_SIZE)\n",
    "# 테스트 데이터셋 생성\n",
    "test_data_encoded = data_encode(test_input_ids, test_attention_masks, test_token_type_ids, y_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 학습 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "with tf.device('/GPU:0'):\n",
    "    BERT_model = TFBertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    )\n",
    "\n",
    "    # 올바른 패키지에서 가져온 옵티마이저 사용\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    BERT_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    # 모델 훈련\n",
    "    NUM_EPOCHS = 5\n",
    "    history = BERT_model.fit(train_data_encoded, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=test_data_encoded)\n",
    "\n",
    "# 모델 저장\n",
    "BERT_model.save(\"bert_model_fashion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109482240 (417.64 MB)\n",
      "Trainable params: 109482240 (417.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "loaded_model = tf.keras.models.load_model(\"bert_model_fashion\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 화장품 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result[result['Domain']== '화장품']\n",
    "# train_test_split\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['RawText'], df['label'], test_size=0.2, random_state=0)\n",
    "\n",
    "# 레이블을 정수형으로 변환\n",
    "y_train = [int(label) for label in y_train]\n",
    "y_test = [int(label) for label in y_test]\n",
    "\n",
    "# 레이블을 넘파이 배열로 변환\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train_texts.shape, X_test_texts.shape, y_train.shape, y_test.shape)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터 토큰화\n",
    "train_input_ids, train_attention_masks, train_token_type_ids = encode(X_train_texts, tokenizer)\n",
    "#테스트데이터 토큰화\n",
    "test_input_ids, test_attention_masks, test_token_type_ids = encode(X_test_texts, tokenizer)\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "train_data_encoded = data_encode(train_input_ids, train_attention_masks, train_token_type_ids, y_train).shuffle(10000).batch(BATCH_SIZE)\n",
    "# 테스트 데이터셋 생성\n",
    "test_data_encoded = data_encode(test_input_ids, test_attention_masks, test_token_type_ids, y_test).batch(BATCH_SIZE)\n",
    "\n",
    "# 모델 정의\n",
    "with tf.device('/GPU:0'):\n",
    "    BERT_model = TFBertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    )\n",
    "\n",
    "    # 올바른 패키지에서 가져온 옵티마이저 사용\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    BERT_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    # 모델 훈련\n",
    "    NUM_EPOCHS = 5\n",
    "    history = BERT_model.fit(train_data_encoded, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=test_data_encoded)\n",
    "\n",
    "# 모델 저장\n",
    "BERT_model.save(\"bert_model_fashion\")\n",
    "\n",
    "# 모델 불러오기\n",
    "loaded_model = tf.keras.models.load_model(\"bert_model_fashion\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처\n",
    "# https://yeong-jin-data-blog.tistory.com/entry/BERT%EB%A1%9C-%EC%98%81%ED%99%94-%EB%A6%AC%EB%B7%B0-%EA%B0%90%EC%84%B1-%EB%B6%84%EC%84%9D%ED%95%98%EA%B8%B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babel==2.11.0\n",
      "brotli==1.0.9\n",
      "jpype1==1.5.0\n",
      "jinja2==3.1.2\n",
      "markdown==3.6\n",
      "markupsafe==2.1.3\n",
      "pyqt5==5.15.10\n",
      "pyqt5-sip==12.13.0\n",
      "pysocks==1.7.1\n",
      "pyyaml==6.0.1\n",
      "pygments==2.15.1\n",
      "qtpy==2.4.1\n",
      "send2trash==1.8.2\n",
      "absl-py==2.1.0\n",
      "anyio==3.5.0\n",
      "archspec==0.2.1\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "asttokens==2.0.5\n",
      "astunparse==1.6.3\n",
      "async-lru==2.0.4\n",
      "attrs==23.1.0\n",
      "beautifulsoup4==4.12.2\n",
      "bleach==4.1.0\n",
      "boltons==23.0.0\n",
      "certifi==2023.11.17\n",
      "cffi==1.16.0\n",
      "charset-normalizer==2.0.4\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "comm==0.1.2\n",
      "conda==23.11.0\n",
      "conda-content-trust==0.2.0\n",
      "conda-libmamba-solver==23.12.0\n",
      "conda-package-handling==2.2.0\n",
      "conda-package-streaming==0.9.0\n",
      "contourpy==1.2.0\n",
      "cryptography==41.0.7\n",
      "cycler==0.12.1\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "distro==1.8.0\n",
      "executing==0.8.3\n",
      "fastjsonschema==2.16.2\n",
      "filelock==3.13.1\n",
      "flatbuffers==23.5.26\n",
      "fonttools==4.47.2\n",
      "fsspec==2024.2.0\n",
      "gast==0.5.4\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.62.1\n",
      "h5py==3.10.0\n",
      "huggingface-hub==0.22.2\n",
      "idna==3.4\n",
      "ipykernel==6.25.0\n",
      "ipython==8.20.0\n",
      "ipywidgets==8.0.4\n",
      "jedi==0.18.1\n",
      "joblib==1.3.2\n",
      "json5==0.9.6\n",
      "jsonpatch==1.32\n",
      "jsonpointer==2.1\n",
      "jsonschema==4.19.2\n",
      "jsonschema-specifications==2023.7.1\n",
      "jupyter==1.0.0\n",
      "jupyter-client==8.6.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-core==5.5.0\n",
      "jupyter-events==0.8.0\n",
      "jupyter-lsp==2.2.0\n",
      "jupyter-server==2.10.0\n",
      "jupyter-server-terminals==0.4.4\n",
      "jupyterlab==4.0.8\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==2.25.1\n",
      "jupyterlab-widgets==3.0.9\n",
      "keras==3.1.1\n",
      "kiwisolver==1.4.5\n",
      "konlpy==0.6.0\n",
      "libclang==18.1.1\n",
      "libmambapy==1.5.3\n",
      "lightgbm==4.2.0\n",
      "lxml==5.1.1\n",
      "markdown-it-py==3.0.0\n",
      "matplotlib==3.8.2\n",
      "matplotlib-inline==0.1.6\n",
      "mdurl==0.1.2\n",
      "mediapipe==0.10.9\n",
      "menuinst==2.0.1\n",
      "mistune==2.0.4\n",
      "mkl-fft==1.3.8\n",
      "mkl-random==1.2.4\n",
      "mkl-service==2.4.0\n",
      "ml-dtypes==0.3.2\n",
      "mpmath==1.3.0\n",
      "namex==0.0.7\n",
      "nbclient==0.8.0\n",
      "nbconvert==7.10.0\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.2.1\n",
      "nltk==3.8.1\n",
      "notebook==7.0.6\n",
      "notebook-shim==0.2.3\n",
      "numpy==1.26.3\n",
      "opencv-contrib-python==4.9.0.80\n",
      "opencv-python==4.9.0.80\n",
      "opt-einsum==3.3.0\n",
      "optree==0.11.0\n",
      "overrides==7.4.0\n",
      "packaging==23.1\n",
      "pandas==2.1.4\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "pillow==10.2.0\n",
      "pip==23.3.1\n",
      "platformdirs==3.10.0\n",
      "pluggy==1.0.0\n",
      "ply==3.11\n",
      "prometheus-client==0.14.1\n",
      "prompt-toolkit==3.0.43\n",
      "protobuf==3.20.3\n",
      "psutil==5.9.0\n",
      "pure-eval==0.2.2\n",
      "pyopenssl==23.2.0\n",
      "pycosat==0.6.6\n",
      "pycparser==2.21\n",
      "pyparsing==3.1.1\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "pytz==2023.3.post1\n",
      "pywin32==305.1\n",
      "pywinpty==2.0.10\n",
      "pyzmq==25.1.0\n",
      "qtconsole==5.5.0\n",
      "referencing==0.30.2\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.7.1\n",
      "rpds-py==0.10.6\n",
      "ruamel.yaml==0.17.21\n",
      "safetensors==0.4.2\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.11.4\n",
      "seaborn==0.13.1\n",
      "setuptools==68.2.2\n",
      "sip==6.7.12\n",
      "six==1.16.0\n",
      "sniffio==1.2.0\n",
      "sounddevice==0.4.6\n",
      "soupsieve==2.5\n",
      "stack-data==0.2.0\n",
      "sympy==1.12\n",
      "tensorboard==2.16.2\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.16.1\n",
      "tensorflow-intel==2.16.1\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.4.0\n",
      "terminado==0.17.1\n",
      "tf-keras==2.16.0\n",
      "threadpoolctl==3.2.0\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.15.2\n",
      "torch==2.2.0\n",
      "torchaudio==2.2.0\n",
      "torchvision==0.17.0\n",
      "tornado==6.3.3\n",
      "tqdm==4.65.0\n",
      "traitlets==5.7.1\n",
      "transformers==4.39.2\n",
      "truststore==0.8.0\n",
      "typing-extensions==4.9.0\n",
      "tzdata==2023.4\n",
      "urllib3==1.26.18\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==0.58.0\n",
      "werkzeug==3.0.1\n",
      "wheel==0.41.2\n",
      "widgetsnbextension==4.0.5\n",
      "win-inet-pton==1.1.0\n",
      "wordcloud==1.9.3\n",
      "wrapt==1.16.0\n",
      "zstandard==0.19.0\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# 설치된 패키지 목록 불러오기\n",
    "installed_packages = pkg_resources.working_set\n",
    "\n",
    "# 패키지 이름과 버전 출력\n",
    "for package in installed_packages:\n",
    "    print(f\"{package.key}=={package.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "babel==2.11.0\r\n",
    "brotli==1.0.9\r\n",
    "jpype1==1.5.0\r\n",
    "jinja2==3.1.2\r\n",
    "markdown==3.6\r\n",
    "markupsafe==2.1.3\r\n",
    "pyqt5==5.15.10\r\n",
    "pyqt5-sip==12.13.0\r\n",
    "pysocks==1.7.1\r\n",
    "pyyaml==6.0.1\r\n",
    "pygments==2.15.1\r\n",
    "qtpy==2.4.1\r\n",
    "send2trash==1.8.2\r\n",
    "absl-py==2.1.0\r\n",
    "anyio==3.5.0\r\n",
    "archspec==0.2.1\r\n",
    "argon2-cffi==21.3.0\r\n",
    "argon2-cffi-bindings==21.2.0\r\n",
    "asttokens==2.0.5\r\n",
    "astunparse==1.6.3\r\n",
    "async-lru==2.0.4\r\n",
    "attrs==23.1.0\r\n",
    "beautifulsoup4==4.12.2\r\n",
    "bleach==4.1.0\r\n",
    "boltons==23.0.0\r\n",
    "certifi==2023.11.17\r\n",
    "cffi==1.16.0\r\n",
    "charset-normalizer==2.0.4\r\n",
    "click==8.1.7\r\n",
    "colorama==0.4.6\r\n",
    "comm==0.1.2\r\n",
    "conda==23.11.0\r\n",
    "conda-content-trust==0.2.0\r\n",
    "conda-libmamba-solver==23.12.0\r\n",
    "conda-package-handling==2.2.0\r\n",
    "conda-package-streaming==0.9.0\r\n",
    "contourpy==1.2.0\r\n",
    "cryptography==41.0.7\r\n",
    "cycler==0.12.1\r\n",
    "debugpy==1.6.7\r\n",
    "decorator==5.1.1\r\n",
    "defusedxml==0.7.1\r\n",
    "distro==1.8.0\r\n",
    "executing==0.8.3\r\n",
    "fastjsonschema==2.16.2\r\n",
    "filelock==3.13.1\r\n",
    "flatbuffers==23.5.26\r\n",
    "fonttools==4.47.2\r\n",
    "fsspec==2024.2.0\r\n",
    "gast==0.5.4\r\n",
    "google-pasta==0.2.0\r\n",
    "grpcio==1.62.1\r\n",
    "h5py==3.10.0\r\n",
    "huggingface-hub==0.22.2\r\n",
    "idna==3.4\r\n",
    "ipykernel==6.25.0\r\n",
    "ipython==8.20.0\r\n",
    "ipywidgets==8.0.4\r\n",
    "jedi==0.18.1\r\n",
    "joblib==1.3.2\r\n",
    "json5==0.9.6\r\n",
    "jsonpatch==1.32\r\n",
    "jsonpointer==2.1\r\n",
    "jsonschema==4.19.2\r\n",
    "jsonschema-specifications==2023.7.1\r\n",
    "jupyter==1.0.0\r\n",
    "jupyter-client==8.6.0\r\n",
    "jupyter-console==6.6.3\r\n",
    "jupyter-core==5.5.0\r\n",
    "jupyter-events==0.8.0\r\n",
    "jupyter-lsp==2.2.0\r\n",
    "jupyter-server==2.10.0\r\n",
    "jupyter-server-terminals==0.4.4\r\n",
    "jupyterlab==4.0.8\r\n",
    "jupyterlab-pygments==0.1.2\r\n",
    "jupyterlab-server==2.25.1\r\n",
    "jupyterlab-widgets==3.0.9\r\n",
    "keras==3.1.1\r\n",
    "kiwisolver==1.4.5\r\n",
    "konlpy==0.6.0\r\n",
    "libclang==18.1.1\r\n",
    "libmambapy==1.5.3\r\n",
    "lightgbm==4.2.0\r\n",
    "lxml==5.1.1\r\n",
    "markdown-it-py==3.0.0\r\n",
    "matplotlib==3.8.2\r\n",
    "matplotlib-inline==0.1.6\r\n",
    "mdurl==0.1.2\r\n",
    "mediapipe==0.10.9\r\n",
    "menuinst==2.0.1\r\n",
    "mistune==2.0.4\r\n",
    "mkl-fft==1.3.8\r\n",
    "mkl-random==1.2.4\r\n",
    "mkl-service==2.4.0\r\n",
    "ml-dtypes==0.3.2\r\n",
    "mpmath==1.3.0\r\n",
    "namex==0.0.7\r\n",
    "nbclient==0.8.0\r\n",
    "nbconvert==7.10.0\r\n",
    "nbformat==5.9.2\r\n",
    "nest-asyncio==1.5.6\r\n",
    "networkx==3.2.1\r\n",
    "nltk==3.8.1\r\n",
    "notebook==7.0.6\r\n",
    "notebook-shim==0.2.3\r\n",
    "numpy==1.26.3\r\n",
    "opencv-contrib-python==4.9.0.80\r\n",
    "opencv-python==4.9.0.80\r\n",
    "opt-einsum==3.3.0\r\n",
    "optree==0.11.0\r\n",
    "overrides==7.4.0\r\n",
    "packaging==23.1\r\n",
    "pandas==2.1.4\r\n",
    "pandocfilters==1.5.0\r\n",
    "parso==0.8.3\r\n",
    "pillow==10.2.0\r\n",
    "pip==23.3.1\r\n",
    "platformdirs==3.10.0\r\n",
    "pluggy==1.0.0\r\n",
    "ply==3.11\r\n",
    "prometheus-client==0.14.1\r\n",
    "prompt-toolkit==3.0.43\r\n",
    "protobuf==3.20.3\r\n",
    "psutil==5.9.0\r\n",
    "pure-eval==0.2.2\r\n",
    "pyopenssl==23.2.0\r\n",
    "pycosat==0.6.6\r\n",
    "pycparser==2.21\r\n",
    "pyparsing==3.1.1\r\n",
    "python-dateutil==2.8.2\r\n",
    "python-json-logger==2.0.7\r\n",
    "pytz==2023.3.post1\r\n",
    "pywin32==305.1\r\n",
    "pywinpty==2.0.10\r\n",
    "pyzmq==25.1.0\r\n",
    "qtconsole==5.5.0\r\n",
    "referencing==0.30.2\r\n",
    "regex==2023.12.25\r\n",
    "requests==2.31.0\r\n",
    "rfc3339-validator==0.1.4\r\n",
    "rfc3986-validator==0.1.1\r\n",
    "rich==13.7.1\r\n",
    "rpds-py==0.10.6\r\n",
    "ruamel.yaml==0.17.21\r\n",
    "safetensors==0.4.2\r\n",
    "scikit-learn==1.3.2\r\n",
    "scipy==1.11.4\r\n",
    "seaborn==0.13.1\r\n",
    "setuptools==68.2.2\r\n",
    "sip==6.7.12\r\n",
    "six==1.16.0\r\n",
    "sniffio==1.2.0\r\n",
    "sounddevice==0.4.6\r\n",
    "soupsieve==2.5\r\n",
    "stack-data==0.2.0\r\n",
    "sympy==1.12\r\n",
    "tensorboard==2.16.2\r\n",
    "tensorboard-data-server==0.7.2\r\n",
    "tensorflow==2.16.1\r\n",
    "tensorflow-intel==2.16.1\r\n",
    "tensorflow-io-gcs-filesystem==0.31.0\r\n",
    "termcolor==2.4.0\r\n",
    "terminado==0.17.1\r\n",
    "tf-keras==2.16.0\r\n",
    "threadpoolctl==3.2.0\r\n",
    "tinycss2==1.2.1\r\n",
    "tokenizers==0.15.2\r\n",
    "torch==2.2.0\r\n",
    "torchaudio==2.2.0\r\n",
    "torchvision==0.17.0\r\n",
    "tornado==6.3.3\r\n",
    "tqdm==4.65.0\r\n",
    "traitlets==5.7.1\r\n",
    "transformers==4.39.2\r\n",
    "truststore==0.8.0\r\n",
    "typing-extensions==4.9.0\r\n",
    "tzdata==2023.4\r\n",
    "urllib3==1.26.18\r\n",
    "wcwidth==0.2.5\r\n",
    "webencodings==0.5.1\r\n",
    "websocket-client==0.58.0\r\n",
    "werkzeug==3.0.1\r\n",
    "wheel==0.41.2\r\n",
    "widgetsnbextension==4.0.5\r\n",
    "win-inet-pton==1.1.0\r\n",
    "wordcloud==1.9.3\r\n",
    "wrapt==1.16.0\r\n",
    "zstandard==0.19.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
